{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02c6c2-b2f2-4fa4-a28e-12e602aa6efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install -U sentence-transformers\n",
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install -U bitsandbytes\n",
    "!pip install flash-attn --no-build-isolation\n",
    "pip install bert-score\n",
    "!pip install FlagEmbedding\n",
    "!pip install accelerate -U\n",
    "!pip install --upgrade huggingface-hub==0.24.0\n",
    "!pip install transformers==4.46.3\n",
    "!pip install bm25s[full]\n",
    "!pip install PyStemmer\n",
    "!pip install jax[cpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a36d328-fd05-44fb-82eb-2f4797b0050c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 00:03:55.112816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-14 00:03:55.112882: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-14 00:03:55.113971: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-14 00:03:55.119756: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-14 00:03:55.990599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790ee68477954e95865a38bc2e043d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = \"t-tech/T-pro-it-1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ").eval()\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a044b2f-fa35-4aab-8857-2f63702f6eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 1300\n",
    "chunk_overlap = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35b5d32-b3f0-483e-9b1d-df4ccb6ce087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "import textwrap\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig\n",
    "import torch\n",
    "from FlagEmbedding import FlagReranker\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "splitter_classic = SentenceSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    sentence_endings = r'(?<=[.!?]) +'\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def split_into_chunks(doc, splitter):\n",
    "    return splitter.split_text(doc)\n",
    "\n",
    "def get_splitted(documents, splitter):\n",
    "    output_with_indices = []\n",
    "    for index, doc in enumerate(documents):\n",
    "        chunks = split_into_chunks(doc, splitter)\n",
    "        for chunk in chunks:\n",
    "            output_with_indices.append((index, chunk))\n",
    "    return output_with_indices\n",
    "\n",
    "def rate(indexes, texts, ind, true_label):\n",
    "    if indexes[ind] == true_label:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def rate_retriev(train, pipeline, metric=None):\n",
    "    trues = 0\n",
    "    for index, row in train.iterrows():\n",
    "        if index % 50 == 0:\n",
    "            print(index)\n",
    "            \n",
    "        q, y = row['question'], np.where(row['text'] == corpus)[0]\n",
    "        txts = new_pipe(indexes, texts, q, k1=50, k2=5, p1=embedder_p, p2=get_top_k_chuks_rerank)\n",
    "        trues += any([rate(indexes, texts, j, y) for j in txts])\n",
    "        if not(any([rate(indexes, texts, j, y) for j in txts])):\n",
    "            logs.append(\n",
    "                [\n",
    "                    index,\n",
    "                    y,\n",
    "                    txts\n",
    "                ]\n",
    "            )\n",
    "    return trues / len(train)\n",
    "\n",
    "def get_top_k_chuks_rerank(question, chunks_e, k=100, batch_size=10):\n",
    "    scores = torch.tensor(reranker.compute_score([[question, chnk] for chnk in chunks_e], normalize=True) )\n",
    "    reranged = reversed(scores.argsort())[:k]\n",
    "    return reranged, scores[reranged] #индексы доков по убыванию релев, скоры\n",
    "def embedder_p(q, chunks, k):\n",
    "    res = retrieve_context_topk(embedder_model, chunks, q, k)\n",
    "    return res, []\n",
    "def retrieve_context_topk(embedder, texts, question, k=1, get_indices=False):\n",
    "    question_embedding = embedder([question])[0]\n",
    "    scores = np.dot(embeddings, question_embedding) / \\\n",
    "        (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(question_embedding))\n",
    "    top_k_indices = np.argsort(scores)[-k:]\n",
    "    if get_indices:\n",
    "        return [i for i in reversed(top_k_indices)]\n",
    "    return [i for i in reversed(top_k_indices)]\n",
    "\n",
    "def get_embedder(model_name):\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "\n",
    "    def f(texts, **args):\n",
    "        # Ensure texts is a list\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]  # If a single string is provided, convert to list\n",
    "        return embedder.encode(texts, normalize_embeddings=True, **args)\n",
    "    \n",
    "    return f\n",
    "\n",
    "import re\n",
    "def parse_questions(output):\n",
    "    output = output.lower()\n",
    "    question_pattern = r'вопрос \\d+ *:.+'\n",
    "    questions = re.findall(question_pattern, output)\n",
    "    questions = [q[q.find(':')+1:].strip() for q in questions]\n",
    "    return questions\n",
    "\n",
    "\n",
    "def create_queries(model, tokenizer, generation_config, question):\n",
    "    prompt_desired = f'''На основе вопроса: {question} создай 2-3 абсолютно вопроса схожих по смыслу, которые могли бы быть заданы в том же контексте но переписаны другими словами\\n\n",
    "                         Формат: Вопрос 1: ...\\n Вопрос 2: ...\\n Вопрос 3: ...'''\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_desired\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    try:\n",
    "        questions_cands = parse_questions(output)\n",
    "        questions_cands[-1] = question  \n",
    "    except (Exception) as e:\n",
    "        print(e)\n",
    "        return [question]\n",
    "    # print('Вопросы сгенерированы')\n",
    "    return questions_cands\n",
    "\n",
    "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
    "    scores = {}\n",
    "    for rank_list in ranked_lists:\n",
    "        for rank, item in enumerate(rank_list):\n",
    "            if item not in scores:\n",
    "                scores[item] = 0\n",
    "            scores[item] += 1 / (rank + 1 + k)\n",
    "    # print('Ранжирование пройдено')\n",
    "    # print(sorted(scores.items(), key=lambda x: x[1], reverse=True))\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def check_real(arr):\n",
    "    return [indexes[el] for el in arr]\n",
    "\n",
    "# def new_pipe(inds, texts, q, k1, k2, p1, p2):\n",
    "#     qs = create_queries(\n",
    "#         model,\n",
    "#         tokenizer, \n",
    "#         generation_config,\n",
    "#         q\n",
    "#     )\n",
    "#     list_of_ranks = []\n",
    "#     for cand_q in qs:\n",
    "#         indexes_f, _ = p1(cand_q, texts, k1)\n",
    "#         template = texts[indexes_f]\n",
    "#         ind2, _ = p2(cand_q, template, k2)\n",
    "#         relev_chunks = template[ind2]\n",
    "#         final_inds = [np.where(texts==rr)[0][0] for rr in relev_chunks]\n",
    "#         list_of_ranks.append(final_inds)\n",
    "#     #потому что (ind, score)\n",
    "#     result_inds = [f.item() for f, _ in reciprocal_rank_fusion(list_of_ranks)][:k2]\n",
    "#     return result_inds\n",
    "\n",
    "def new_pipe(inds, texts, q, k1, k2, p1, p2, scores=False):\n",
    "    # qs = create_queries(\n",
    "    #     model,\n",
    "    #     tokenizer, \n",
    "    #     generation_config,\n",
    "    #     q\n",
    "    # )\n",
    "    \n",
    "    qs = [q]\n",
    "    list_of_ranks = []\n",
    "    for cand_q in qs:\n",
    "        indexes_f, _ = p1(cand_q, texts, k1)\n",
    "        indexes_bm25, _ = get_top_k(cand_q, 1321321, k1)\n",
    "        \n",
    "        indexes_f = [f.item() for f, _ in reciprocal_rank_fusion(\n",
    "            [indexes_f, indexes_bm25]\n",
    "        )][:k1]\n",
    "        \n",
    "        template = texts[indexes_f]\n",
    "        ind2, scorings_reranker = p2(cand_q, template, k2)\n",
    "        relev_chunks = template[ind2]\n",
    "        final_inds = [np.where(texts==rr)[0][0] for rr in relev_chunks]\n",
    "        list_of_ranks.append(final_inds)\n",
    "    #потому что (ind, score)\n",
    "    if scores:\n",
    "        return list_of_ranks[0], scorings_reranker\n",
    "    return list_of_ranks[0], []\n",
    "\n",
    "\n",
    "with open('webhelp_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "if 'data' in data:\n",
    "    df = pd.DataFrame(data['data'])\n",
    "\n",
    "df = df[df.description.str.len() > 4]\n",
    "\n",
    "df['text'] = df['title'] + ' ' + df['description']\n",
    "\n",
    "texts_urls = df[['text', 'url']].drop_duplicates(subset='text', keep='first').reset_index(drop=True)\n",
    "corpus = texts_urls.text.values\n",
    "urls = texts_urls.url.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad4b2175-a2cd-4460-b0db-bd40cde263e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2024-12-13 21:31:48,747:jax._src.xla_bridge:969: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import Stemmer\n",
    "import bm25s\n",
    "corpus = texts_urls.text.values\n",
    "urls = texts_urls.url.values\n",
    "\n",
    "corpus1 = get_splitted(corpus, splitter_classic)\n",
    "indexes, texts = np.array([f for f, _ in corpus1]), np.array([s for _, s in corpus1])\n",
    "\n",
    "stemmer = Stemmer.Stemmer(\"russian\")\n",
    "\n",
    "\n",
    "corpus_tokens = bm25s.tokenize(texts, stemmer=stemmer, stopwords=\"ru\")\n",
    "retriever = bm25s.BM25(method=\"bm25+\", delta=1.5)\n",
    "retriever.index(corpus_tokens)\n",
    "\n",
    "\n",
    "def get_top_k(question, chunks, k) -> tuple[str, str]:\n",
    "    questions_tokens = bm25s.tokenize(question, stemmer=stemmer, stopwords=\"ru\")\n",
    "    result_indexes, scores = retriever.retrieve(questions_tokens, k=k)\n",
    "\n",
    "    result_indexes = result_indexes[0]\n",
    "\n",
    "    return result_indexes, []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9580a-d842-41be-b8b4-fd916ac7a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#КОНТЕКСТУАЛЬНЫЕ ЧАНКИ, воркайте??\n",
    "with open('logical_chunks_new_by_vikhr.json', 'r') as fl:\n",
    "    chunks_logical = json.loads(fl.read())\n",
    "    for ck_id, context_new in chunks_logical:\n",
    "        texts[ck_id] = 'Контекст:' + context_new.lower().replace('контекст:', '') + '\\n' + texts[ck_id]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42d66ea9-a703-4e45-892b-c7aadf572f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) \n",
    "embedder_model = get_embedder('deepvk/USER-bge-m3')\n",
    "embeddings = embedder_model(texts, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "576fcf59-4fcc-47aa-b52c-537a0b321e47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_pipe(indexes, texts, q, k1=50, k2=5, scores=False):\n",
    "    ranked, scorings = new_pipe(indexes, texts, q, k1=k1, k2=k2, p1=embedder_p, p2=get_top_k_chuks_rerank, scores=scores)\n",
    "    ranked_texts = [corpus[ind] for ind in check_real(ranked)]\n",
    "    if scores:\n",
    "        return ranked_texts, [ind for ind in check_real(ranked)], scorings \n",
    "    return ranked_texts, [ind for ind in check_real(ranked)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61971364-e076-4722-93a5-9bd4a96bb7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, generation_config, prompt_desired):\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_desired\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "    print('here')\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(**data, generation_config=generation_config, max_new_tokens=1524)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4489b28b-11c2-4e81-99d7-6fb370829923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAG_PROMPT = (\n",
    "    \"Контекстная информация снизу. (набор документов чанков)\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Используя только данную информацию контекст, \"\n",
    "    \"Ответь на вопрос. Дай прямой ответ. не генерируй markdown. Если этих документов недостаточно, чтобы ответить на вопрос,отказывайся отвечать. Если вопрос не по теме домена сразу отказывайся отвечать.\\n\"\n",
    "    \"Вопрос: {query_str}\\n\"\n",
    "    \"Ответ: \"\n",
    ")\n",
    "\n",
    "REASONING_PROMPT = (\n",
    "        \"Контекстная информация снизу. (набор документов чанков)\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{context_str}\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"Используя только данную информацию контекст и вопрос пользователя, \"\n",
    "        \"Порассуждай сперва какие из этих чанков точно содержут ответ. В последнем предложении выдели (номеры) чанков которые точно содержат ответ. \"\n",
    "        \"чанки идут по релевантности по убыванию.\"\n",
    "        \"Если вопрос подразумевает использование нескольких чанков, также выдели их. В конце дай номер релевантных чанков. Ответ на вопрос не надо, нужно чтобы ты выделил чанки\\n\"\n",
    "        \"Вопрос: {query_str}\\n\"\n",
    "        \"Твое рассуждение: \"\n",
    ")\n",
    "\n",
    "\n",
    "# #не ворк\n",
    "# RAG_PROMPT_1_shot = (\n",
    "#     \"Контекстная информация снизу. (набор документов чанков)\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"{context_str}\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"Используя только данную информацию контекст, \"\n",
    "#     \"Ответь на вопрос. Дай прямой ответ. не генерируй markdown. Если этих документов недостаточно, чтобы ответить на вопрос, отказывайся отвечать.\\n\"\n",
    "#     \"Пример: вопрос:{q_shot}; эталон ответ:{tgt_shot}\"\n",
    "#     \"конец примера.\\n\"\n",
    "#     \"Вопрос: {query_str}\\n\"\n",
    "#     \"Ответ: \"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dca042cc-d343-4c9f-8b58-f9d436493a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def end2end_pipe(user_question, prompt_reasoning=REASONING_PROMPT, prompt=RAG_PROMPT):\n",
    "    texts_orig, inds_orig = call_pipe(indexes, texts, user_question, k1=50, k2=10)\n",
    "    texts_rer = copy.deepcopy(texts_orig)\n",
    "    for j in range(len(texts_rer)):\n",
    "        texts_rer[j] = f'Чанк {j+1}: {texts_rer[j]}'\n",
    "    print(prompt_reasoning)\n",
    "    prompt_desired = prompt_reasoning.format(\n",
    "        context_str = '\\n'.join(texts_rer),\n",
    "        query_str = user_question\n",
    "    )\n",
    "    output_chunks_related = generate(llm, tokenizer, generation_config, prompt_desired)\n",
    "    prompt_desired_rag = prompt.format(\n",
    "        context_str = '\\n'.join(texts_rer),\n",
    "        query_str = user_question\n",
    "    )\n",
    "    output_rag = generate(llm, tokenizer, generation_config, prompt_desired_rag)\n",
    "    return output_chunks_related, output_rag, inds_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6ec04461-9ee1-4553-a239-bde8a822b4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_chunks_true(string, relev_inds):\n",
    "    splitted = [text.lower() for text in string.split('.')[::-1]]\n",
    "    try:\n",
    "        for text in splitted:\n",
    "            relevs = list(map(int, re.findall(r'\\d+', text)))\n",
    "            print(relevs)\n",
    "            if len(relevs) > 0:\n",
    "                print([relev_inds[relev_by_gpt - 1] for relev_by_gpt in relevs])\n",
    "                return list(set([relev_inds[relev_by_gpt - 1] for relev_by_gpt in relevs]))\n",
    "    except(Exception) as ex:\n",
    "        print(ex)\n",
    "        return []\n",
    "    return []\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "049d8898-8a9f-4207-9546-430cad008d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def calc_precision(true_, pred_):\n",
    "    inter = set(true_) & set(pred_)\n",
    "    return len(inter) / len(pred_) \n",
    "\n",
    "def calc_recall(true_, pred_):\n",
    "    inter = set(true_) & set(pred_)\n",
    "    return len(inter) / len(true_)\n",
    "\n",
    "def calc_f_beta(PR, RE, beta=2, eps=0.01):\n",
    "    ch = (1 + beta**2) * (PR * RE)\n",
    "    znam = (beta ** 2) * PR + RE + eps\n",
    "    return ch / znam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ecd2101d-efd9-4bd5-81a1-6cb29de7183d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def take_threshold(\n",
    "        retrieval_indexes,\n",
    "        scores,\n",
    "        corpus,\n",
    "        urls,\n",
    "        thres=0.5\n",
    "    ):\n",
    "    retrieval_indexes = np.array(retrieval_indexes)\n",
    "    scores = np.array(scores)\n",
    "    thresholded = retrieval_indexes[scores > thres]\n",
    "    return thresholded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0cb113e-61ea-4e7d-ad9c-ff42d17bda14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[3, 7, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 7]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_chunks_true('''Для ответа на вопрос о том, как получить КЭП от Тинькофф, если пользователь не находится в Москве, необходимо рассмотреть чанки, содержащие информацию о процессе получения КЭП вне Москвы.\n",
    "\n",
    "1. Чанк 1: Упоминается, что если пользователь не в Москве, он может подать заявку через личный кабинет Тинькофф Бизнеса и получить КЭП в удостоверяющем центре «Основание» в своем регионе.\n",
    "2. Чанк 2: Описывает процесс подачи заявки на КЭП через личный кабинет Тинькофф Бизнеса и получения подписи в филиале «Основание» в другом городе.\n",
    "3. Чанк 3: Подробно описывает процедуру получения КЭП в филиале «Основание» в своем городе, если пользователь не в Москве.\n",
    "4. Чанк 7: Также содержит подробную инструкцию о том, как получить КЭП в филиале «Основание» в другом городе.\n",
    "\n",
    "Чанки 2, 3 и 7 наиболее релевантны, поскольку они предоставляют конкретные шаги для получения КЭП в случае, когда пользователь не находится в Москве. Чанк 1 предоставляет общую информацию, но менее детализирован.\n",
    "\n",
    "Релевантные чанки по убыванию релевантности: 3, 7, 2.''', [2, 2, 2, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ccf974-5e7f-4074-b196-4dbe03ff6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da8041ad-60fa-42e6-b030-05a6ae8a3d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from bert_score import score\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "\n",
    "df = pd.read_csv(\"bench_final.csv\")\n",
    "\n",
    "logs = []\n",
    "final_df = []\n",
    "def benchmark(k = 12):\n",
    "    bert_scores = []\n",
    "    timings = []\n",
    "    hits = 0\n",
    "    gpt_metric = 0\n",
    "    \n",
    "    for i, row in tqdm.tqdm(df.iterrows()):\n",
    "        user_question = row[\"Вопрос\"]\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        output_chunks_related, output_rag, inds_orig = end2end_pipe(user_question)\n",
    "        parsed_urls_ids = parse_chunks_true(output_chunks_related, inds_orig)\n",
    "        urls_relev = [urls[url_id] for url_id in parsed_urls_ids]\n",
    "        final_df.append([user_question, output_rag, urls_relev])\n",
    "        elapsed_time = time.time() - start_time\n",
    "        timings.append(elapsed_time)\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "\n",
    "        logs.append(\n",
    "            {\n",
    "                'i': i,\n",
    "                'output': output_rag,\n",
    "                'model_name': model_name,\n",
    "                'rag_prompt': RAG_PROMPT,\n",
    "                'reasoning': output_chunks_related,\n",
    "                'chunks': list(inds_orig),\n",
    "                'k': 10,\n",
    "                'chunk_overlap':chunk_overlap,\n",
    "                'chunk_size':chunk_size\n",
    "            }\n",
    "        )\n",
    "    #no need\n",
    "    # avg_bert_score = sum(bert_scores) / len(bert_scores)\n",
    "    avg_time = sum(timings) / len(timings)\n",
    "    return _, _, avg_time, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fffd1594-8c00-452b-85dd-68076757a4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config.temperature=0.2\n",
    "results = benchmark(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "423acfa6-7881-4c7a-bc31-0a837c64dbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72898fa8-e56f-4dff-a262-6c34e80112cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(kkk)):\n",
    "    parsed_urls_ids = parse_chunks_true(kkk[i], logs[i]['chunks'])\n",
    "    urls_relev = [urls[url_id] for url_id in parsed_urls_ids]\n",
    "    final_df.loc[i, 'Urls'] = urls_relev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "51a3ee90-288e-4ca4-8ba2-75620082c943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zhiest = pd.DataFrame(yyoy, columns=[\n",
    "    'Question',\n",
    "    'Predict',\n",
    "    'Urls'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "437676df-9079-40ce-baf4-c204bf7801b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zhiest.to_csv('finalOur.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c4420-d29b-4b5b-9f3a-7cebe15a6e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_as_json(filename, data):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(data, file, indent=4) \n",
    "        print(f\"Data successfully written to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing to the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb312f92-1639-416c-a93d-929470a9d0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(logs)):\n",
    "    logs[i]['chunks'] = list(map(int, logs[i]['chunks']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6ed83-49c6-4bcc-adb4-b61091be1fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_as_json('ideal_prompt_100_t_pro_4bit.json', logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad3bb2-fa07-480f-b69c-206ddfb7a50f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#contextual creater\n",
    "def context_creater(chunk, full_chunk):\n",
    "    prompt = 'Тебе даны 2 текста. первый текст взят из второго. Твоя задача дать очень маленький контекст для первого текста на основе второго' \\\n",
    "                'чтобы первый текст был осмысленным. НЕ УПОМЯНАЙ ПЕРВЫЙ ТЕКСТ И НЕ УПОМЯНАЙ ВТОРОЙ ТЕКСТ\"\\n' \\\n",
    "                'Первый текст: {chunk}\\n\\n' \\\n",
    "                'Второй текст: {full_chunk}\\n\\nКонтекст:'\n",
    "    res = generate(model, \n",
    "                   tokenizer, \n",
    "                   generation_config, \n",
    "                   prompt.format(chunk=chunk, full_chunk=full_chunk))\n",
    "    return res    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758e2fd-e5d0-430b-a426-272548be05dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1b94e-56cc-4d3e-bdb3-04011372ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = Counter(indexes)\n",
    "curr = 0\n",
    "chunks_d = []\n",
    "for ind, (k, v) in enumerate(dd):\n",
    "    if v != 1:\n",
    "        for j in range(v):\n",
    "            res = context_creater(texts[curr + j], corpus[indexes[curr + j]])\n",
    "            chunks_d.append((curr + j, res))\n",
    "    curr += v\n",
    "save_as_json('logical_chunks', chunks_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba6d7f0-0923-4dd9-9ef7-6c1e86fc529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#конкатенация контекста.\n",
    "def concat_context(contexts, texts, delim='.'):\n",
    "    for id_chunk, context in contexts:\n",
    "        texts[id_chunk] = context + delim + texts[id_chunk]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
